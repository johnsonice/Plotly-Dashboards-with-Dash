{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient version by using mallet2gensim conversion for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/notebook/poc\n",
      "../../model/mallet_weights_50_2019_01_15\n",
      "../../data/processed/dictionary.dict\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('../../model/mallet_weights_50_2019_01_15')\n",
    "dictionary_path = os.path.join('../../data/processed/dictionary.dict')\n",
    "label_definition_path = os.path.join('../../data/processed/Topic Definition_2019_01_15.npy')\n",
    "text_file_path = \"../../documentation/sample_docs/5138964-v5-Brazil_2013_Article_IV_Consultation_-_Policy_Note.DOCX\"\n",
    "processed_file_path = os.path.join('../../data/processed/', text_file_path.split(sep='/')[-1].split(sep ='.')[0] + '.csv')\n",
    "print(os.getcwd())\n",
    "print(model_path)\n",
    "print(dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Model, Dictionary, and Label (manually created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n",
      "WARNING:root:failed to load state from ../../model/mallet_weights_50_2019_01_15.state: [Errno 2] No such file or directory: '../../model/mallet_weights_50_2019_01_15.state'\n"
     ]
    }
   ],
   "source": [
    "lda_model = LdaModel.load(model_path)\n",
    "old_dict = corpora.Dictionary.load(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_topic_dict = np.load(label_definition_path)\n",
    "label_topic_dict = dict(label_topic_dict.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Topic Dictionary (Topic ID ~ Word List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_topic_list = lda_model.show_topics(num_topics= 50, num_words= 15, formatted= False)\n",
    "model_topic_list = dict(model_topic_list)\n",
    "\n",
    "model_topic_dict = dict()\n",
    "\n",
    "for key, value in model_topic_list.items():\n",
    "    word_list, prob = zip(*value)\n",
    "    model_topic_dict[key] = list(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Topic-Lable Mapping by applying IOU to manually-created labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_intersection_over_union(list_a, list_b):\n",
    "    \n",
    "    inter_set = list(set(list_a) & set(list_b))\n",
    "    union_set = list(set(list_a) | set(list_b))\n",
    "    \n",
    "    return len(inter_set)/len(union_set)\n",
    "\n",
    "def map_topic_label(model_dict, label_dict):\n",
    "    \n",
    "    new_list =dict()\n",
    "    \n",
    "    for model_key, model_value in model_dict.items():\n",
    "        iou_list =[]\n",
    "        for label_key, label_value in label_dict.items():\n",
    "            iou_list.append(calculate_intersection_over_union(model_value, label_value))\n",
    "        max_id = np.array(iou_list).argmax()\n",
    "        new_list[model_key] = list(label_dict.keys())[max_id]\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "topic_label_dict = map_topic_label(model_dict= model_topic_dict, label_dict= label_topic_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Policies. With supply-side constraints restraining short-term growth, the overall policy stance should gradually withdraw stimulus, embed policies in Brazilâ€™s long-standing frameworks, and emphasize monetary policy as the main counter-cyclical tool. Exchange rate flexibility and judicious use of CFMs can continue to help address the challenges posed by volatile capital flows. The renewed focus on supply-side policies is welcome but further efforts are needed to mobilize domestic saving, increase investment, and enhance productivity and competitiveness.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_doc(f_path,word_length_filter=20):\n",
    "    if os.path.isfile(f_path):\n",
    "        doc = Document(f_path)\n",
    "        text_list = [p.text for p in doc.paragraphs if len(p.text)>10]#[3:]\n",
    "        text_list = [p.replace('\\xa0',' ') for p in text_list] # some clean up \n",
    "        text_list = [p for p in text_list if len(p.split()) > word_length_filter]\n",
    "    else:\n",
    "        raise Exception('File does not exist: {}'.format(f_path))\n",
    "\n",
    "    return text_list\n",
    "\n",
    "new_text = read_doc(text_file_path)\n",
    "\n",
    "new_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en') \n",
    "\n",
    "def infer_single_paragraph(paragraph, ldaModel):\n",
    "    '''Load raw paragraph and model, return cleaned paragraph and topic_label with highest probability'''\n",
    "    #### Process text using Spacy for Tokenization/Lemmentization and loaded dictionary for bag-of-words\n",
    "    new_text = nlp(paragraph)\n",
    "    new_doc = [word.lemma_ for word in new_text]\n",
    "    new_bow = old_dict.doc2bow(new_doc)\n",
    "    \n",
    "    ## Make inference using gensim_lda model (converted from mallet) and retrieve Top ID\n",
    "    ldaModel = models.wrappers.ldamallet.malletmodel2ldamodel(ldaModel)\n",
    "    topic_prob = ldaModel[new_bow]\n",
    "    n, prob = zip(*topic_prob)\n",
    "    top_id = np.array(n)[np.array(prob).argmax()]\n",
    "    \n",
    "    return new_text, top_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.17311954498291 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "result = [infer_single_paragraph(paragraph, lda_model) for paragraph in new_text]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, topic_id = zip(*result)\n",
    "\n",
    "result = pd.DataFrame({'Paragraph': p, 'Topic ID' : topic_id})\n",
    "\n",
    "result.Paragraph.apply(str)\n",
    "result['Label'] = result['Topic ID'].apply(lambda x: topic_label_dict[x])\n",
    "\n",
    "result.to_csv(processed_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
